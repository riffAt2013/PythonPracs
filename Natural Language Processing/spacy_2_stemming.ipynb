{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "spacy -2 stemming.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tkk_B1wxWJ5",
        "colab_type": "text"
      },
      "source": [
        "# Stemming\n",
        "\n",
        "### Why we can't use spacy here\n",
        "Often when searching text for a certain keyword, it helps if the search returns variations of the word. For instance, searching for \"boat\" might also return \"boats\" and \"boating\". Here, \"boat\" would be the **stem** for [boat, boater, boating, boats].\n",
        "\n",
        "Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required. In fact, spaCy doesn't include a stemmer, opting instead to rely entirely on lemmatization. For those interested, there's some background on this decision [here](https://github.com/explosion/spaCy/issues/327). We discuss the virtues of *lemmatization* in the next section.\n",
        "\n",
        "Instead, we'll use another popular NLP tool called **nltk**, which stands for *Natural Language Toolkit*. For more information on nltk visit https://www.nltk.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txn8unr1xWJ7",
        "colab_type": "text"
      },
      "source": [
        "## Porter Stemmer\n",
        "\n",
        "One of the most common - and effective - stemming tools is [*Porter's Algorithm*](https://tartarus.org/martin/PorterStemmer/) developed by Martin Porter in [1980](https://tartarus.org/martin/PorterStemmer/def.txt). The algorithm employs five phases of word reduction, each with its own set of mapping rules. In the first phase, simple suffix mapping rules are defined, such as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1b-e-zoxWJ-",
        "colab_type": "text"
      },
      "source": [
        "![Imgur](https://i.imgur.com/HHfHcNk.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1i6YI3uxWJ_",
        "colab_type": "text"
      },
      "source": [
        "From a given set of stemming rules only one rule is applied, based on the longest suffix S1. Thus, `caresses` reduces to `caress` but not `cares`.\n",
        "\n",
        "More sophisticated phases consider the length/complexity of the word before applying a rule. For example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuyWxgjrxWKA",
        "colab_type": "text"
      },
      "source": [
        "![Imgur](https://i.imgur.com/S7dF5aU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSxzZxIAxWKA",
        "colab_type": "text"
      },
      "source": [
        "Here `m>0` describes the \"measure\" of the stem, such that the rule is applied to all but the most basic stems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEvhMp1_xWKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the toolkit and the full Porter Stemmer library\n",
        "import nltk\n",
        "from nltk.stem.porter import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk5SeYi9xWKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a porter stemmer object that performs stemming\n",
        "# 3 variants of Porter stemmer can be used. By default it runs NLTK EXTENSIONS\n",
        "# we can use ORIGINAL_ALGORITHM or MARTIN_EXTENSIONS\n",
        "p_stemmer = PorterStemmer()\n",
        "o_stemmer = PorterStemmer(mode = \"ORIGINAL_ALGORITHM\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zieryx6XxWKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ['run','runner','running','ran','runs','easily','fairly', 'skies']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7zzuShfxWKL",
        "colab_type": "code",
        "outputId": "0e9ea77e-1f84-40fb-e4ed-e9aae840f34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "# notice how skies changed to ski in OGALGO implementation of the stemmer\n",
        "# this is because it is in the list of irregular words that doesnt fall quite \n",
        "# right in the algorithm rules. its fixed in later algo revisions such as NLTK_EXTENSIONS\n",
        "\n",
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))\n",
        "    print(f\"{word} stemmed using og algo --> {o_stemmer.stem(word)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run --> run\n",
            "run stemmed using og algo --> run\n",
            "runner --> runner\n",
            "runner stemmed using og algo --> runner\n",
            "running --> run\n",
            "running stemmed using og algo --> run\n",
            "ran --> ran\n",
            "ran stemmed using og algo --> ran\n",
            "runs --> run\n",
            "runs stemmed using og algo --> run\n",
            "easily --> easili\n",
            "easily stemmed using og algo --> easili\n",
            "fairly --> fairli\n",
            "fairly stemmed using og algo --> fairli\n",
            "skies --> sky\n",
            "skies stemmed using og algo --> ski\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUhj76acxWKO",
        "colab_type": "text"
      },
      "source": [
        "**Note** how the stemmer recognizes \"runner\" as a noun, not a verb form or participle. Also, the adverbs \"easily\" and \"fairly\" are stemmed to the unusual root \"easili\" and \"fairli\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jff9Jd7nxWKP",
        "colab_type": "text"
      },
      "source": [
        "## Snowball Stemmer\n",
        "This is somewhat of a misnomer, as Snowball is the name of a stemming language developed by Martin Porter. The algorithm used here is more acurately called the \"English Stemmer\" or \"Porter2 Stemmer\". It offers a slight improvement over the original Porter stemmer, both in logic and speed. Since **nltk** uses the name SnowballStemmer, we'll use it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GBtBMsRxWKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# The Snowball Stemmer requires that you pass a language parameter\n",
        "s_stemmer = SnowballStemmer(language='english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpTkKs3LxWKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "# words = ['generous','generation','generously','generate']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ8dXZHgxWKW",
        "colab_type": "code",
        "outputId": "cc3372df-2a78-4091-cf7e-5fba54322a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "for word in words:\n",
        "    print(word+' --> '+s_stemmer.stem(word))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "YGo98_ecxWKZ",
        "colab_type": "text"
      },
      "source": [
        "In this case the stemmer performed the same as the Porter Stemmer, with the exception that it handled the stem of \"fairly\" more appropriately with \"fair\"\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAFATgKJxWKk",
        "colab_type": "text"
      },
      "source": [
        "___\n",
        "**Stemming has its drawbacks. If given the token `saw`, stemming might always return `saw`, whereas lemmatization would likely return either `see` or `saw` depending on whether the use of the token was as a verb or a noun. As an example, consider the following:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bRrSn-0xWKl",
        "colab_type": "code",
        "outputId": "acadcae4-31b2-46c4-a53b-ff15a157c76b",
        "colab": {}
      },
      "source": [
        "phrase = 'I am meeting him tomorrow at the meeting'\n",
        "for word in phrase.split():\n",
        "    print(word+' --> '+p_stemmer.stem(word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I --> I\n",
            "am --> am\n",
            "meeting --> meet\n",
            "him --> him\n",
            "tomorrow --> tomorrow\n",
            "at --> at\n",
            "the --> the\n",
            "meeting --> meet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZHq_U2gxWKo",
        "colab_type": "text"
      },
      "source": [
        "Here the word \"meeting\" appears twice - once as a verb, and once as a noun, and yet the stemmer treats both equally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOoyFqGBepLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ffda28b6-c380-4156-f13e-e3fbd9fb63aa"
      },
      "source": [
        "listofwords = ['likelihood', 'textual', 'formatting', 'rigorous']\n",
        "\n",
        "for word in listofwords:\n",
        "  print(f\"{word} using porter stemmer {p_stemmer.stem(word)}\")\n",
        "  print(f\"{word} using snowball(porter2) stemmer {o_stemmer.stem(word)}\", end = '\\n\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "likelihood using porter stemmer likelihood\n",
            "likelihood using snowball(porter2) stemmer likelihood\n",
            "\n",
            "textual using porter stemmer textual\n",
            "textual using snowball(porter2) stemmer textual\n",
            "\n",
            "formatting using porter stemmer format\n",
            "formatting using snowball(porter2) stemmer format\n",
            "\n",
            "rigorous using porter stemmer rigor\n",
            "rigorous using snowball(porter2) stemmer rigor\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb9171o6fggG",
        "colab_type": "text"
      },
      "source": [
        "## **We learned that stemming is an alternative to lemmatization which is by default used by spacy. The main thing here is to find the root of these words\n",
        "## which are morphologically affixed that is modified with extra affixes to create a new word. We just learned stemming is one way to do it thats all**"
      ]
    }
  ]
}